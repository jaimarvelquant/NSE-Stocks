#!/usr/bin/env python3
"""
Nautilus Data Transformation Orchestration CLI.

Thin wrapper that parses CLI arguments and delegates to modular transformers.
No business logic - only wiring and argument parsing.
"""

import sys
import shutil
from pathlib import Path
from datetime import date
from typing import Optional, Set
import argparse
import logging
import os
import posixpath
import tempfile

import paramiko

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Add transformers directory to path for direct imports
TRANSFORMERS_DIR = Path(__file__).parent
if str(TRANSFORMERS_DIR) not in sys.path:
    sys.path.insert(0, str(TRANSFORMERS_DIR))

# Add utils directory to path
UTILS_DIR = PROJECT_ROOT / "utils"
if str(UTILS_DIR) not in sys.path:
    sys.path.insert(0, str(UTILS_DIR))

from nautilus_trader.persistence.catalog.parquet import ParquetDataCatalog

# Import transformers - use direct imports since we added paths
from common import configure_logging
from index_transformer import transform_index_bars
from futures_transformer import transform_futures_bars
from options_transformer import transform_options_bars
from equity_transformer import transform_equity_bars

# Import contract_generators
from contract_generators import set_lot_size_csv_path

logger = logging.getLogger(__name__)


def _sftp_mkdir_p(sftp: paramiko.SFTPClient, remote_directory: str) -> None:
    """
    Recursively create remote directories if they do not exist.
    """
    if remote_directory in ("", "/"):
        return
    try:
        sftp.stat(remote_directory)
        return
    except IOError:
        parent, _ = posixpath.split(remote_directory.rstrip("/"))
        if parent and parent != remote_directory:
            _sftp_mkdir_p(sftp, parent)
        sftp.mkdir(remote_directory)


def download_input_from_ssh(remote_dir: str, local_dir: Path) -> None:
    """
    Download input data from the remote SSH server to a local temporary directory.
    
    Args:
        remote_dir: Remote directory path on SSH server (e.g., "/home/ubuntu/raw_data")
        local_dir: Local temporary directory to download files to
    """
    # SSH server configuration
    host = "192.168.173.175"
    port = 22
    username = "ubuntu"
    password = "data"
    
    # Normalize remote path (ensure forward slashes)
    remote_dir = remote_dir.replace("\\", "/")
    if not remote_dir.startswith("/"):
        remote_dir = "/" + remote_dir
    
    logger.info(f"Downloading input data from {host}:{remote_dir} to {local_dir}")
    
    transport = paramiko.Transport((host, port))
    try:
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)
        
        # Check if remote directory exists
        try:
            sftp.stat(remote_dir)
        except IOError as e:
            logger.error(f"Remote directory does not exist on SSH server: {remote_dir}")
            logger.error(f"Please verify the path exists on {host} as user {username}")
            logger.error(f"Error details: {e}")
            raise FileNotFoundError(f"Remote directory not found: {remote_dir}") from e
        
        # Recursively download directory
        def _download_recursive(remote_path: str, local_path: Path):
            try:
                attrs = sftp.listdir_attr(remote_path)
                local_path.mkdir(parents=True, exist_ok=True)
                
                for attr in attrs:
                    remote_item = posixpath.join(remote_path, attr.filename)
                    local_item = local_path / attr.filename
                    
                    if attr.st_mode & 0o040000:  # Directory
                        _download_recursive(remote_item, local_item)
                    else:  # File
                        logger.debug(f"Downloading {remote_item} -> {local_item}")
                        sftp.get(remote_item, str(local_item))
            except IOError as e:
                logger.error(f"Error accessing {remote_path}: {e}")
                raise
        
        _download_recursive(remote_dir, local_dir)
        sftp.close()
        logger.info(f"Completed download of input data from {host}:{remote_dir}")
    finally:
        transport.close()


def upload_file_to_ssh(local_file: Path, remote_path: str) -> None:
    """
    Upload a single file to the remote SSH server.
    
    Args:
        local_file: Local file path to upload
        remote_path: Remote file path on SSH server
    """
    # SSH server configuration
    host = "192.168.173.175"
    port = 22
    username = "ubuntu"
    password = "data"
    
    if not local_file.exists():
        logger.warning(f"Local file does not exist, nothing to upload: {local_file}")
        return
    
    logger.info(f"Uploading log file {local_file} to {host}:{remote_path}")
    
    transport = paramiko.Transport((host, port))
    try:
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)
        
        # Create remote directory if it doesn't exist
        remote_dir = posixpath.dirname(remote_path)
        if remote_dir:
            _sftp_mkdir_p(sftp, remote_dir)
        
        sftp.put(str(local_file), remote_path)
        sftp.close()
        logger.info(f"Successfully uploaded log file to {host}:{remote_path}")
    finally:
        transport.close()


def upload_output_to_ssh(local_dir: Path, remote_dir: str) -> None:
    """
    Upload the generated Nautilus catalog to the remote SSH server.

    Args:
        local_dir: Local directory containing files to upload
        remote_dir: Remote directory path on SSH server
    """
    # SSH server configuration
    host = "192.168.173.175"
    port = 22
    username = "ubuntu"
    password = "data"

    if not local_dir.exists():
        logger.warning(f"Local output directory does not exist, nothing to upload: {local_dir}")
        return

    logger.info(f"Uploading Nautilus catalog from {local_dir} to {host}:{remote_dir}")

    transport = paramiko.Transport((host, port))
    try:
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)

        local_root_str = str(local_dir)
        for root, dirs, files in os.walk(local_root_str):
            rel_path = os.path.relpath(root, local_root_str)
            if rel_path == ".":
                target_remote_dir = remote_dir
            else:
                target_remote_dir = posixpath.join(remote_dir, rel_path.replace("\\", "/"))

            _sftp_mkdir_p(sftp, target_remote_dir)

            for filename in files:
                local_path = os.path.join(root, filename)
                remote_path = posixpath.join(target_remote_dir, filename)
                logger.debug(f"Uploading {local_path} -> {remote_path}")
                sftp.put(local_path, remote_path)

        sftp.close()
        logger.info(f"Completed upload of Nautilus catalog to {host}:{remote_dir}")
    finally:
        transport.close()


def parse_expiry_dates(expiry_strings: list[str]) -> Optional[Set[date]]:
    """Parse expiry date strings into set of date objects."""
    if not expiry_strings:
        return None
    
    expiry_set = set()
    for exp_str in expiry_strings:
        try:
            expiry_set.add(date.fromisoformat(exp_str))
        except ValueError:
            logger.warning(f"Invalid expiry date format: {exp_str}, skipping")
    
    return expiry_set if expiry_set else None


def main():
    """Main CLI entrypoint."""
    # Input and output paths are on the SSH server
    default_input_dir = Path("/home/ubuntu/raw_data")
    default_output_dir = Path("/home/ubuntu/nautilus_data")
    
    parser = argparse.ArgumentParser(
        description="Transform NSE data to Nautilus catalog (Modular Pattern)"
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=default_input_dir,
        help=f"Input directory with raw data (default: {default_input_dir})"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=default_output_dir,
        help=f"Output directory for Nautilus catalog (default: {default_output_dir})"
    )
    parser.add_argument(
        "--symbols",
        nargs="+",
        default=["NIFTY", "BANKNIFTY"],
        help="Symbols to transform"
    )
    parser.add_argument(
        "--start-date",
        type=str,
        default="2024-01-01",
        help="Start date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--end-date",
        type=str,
        default="2024-01-31",
        help="End date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Clean output directory before starting"
    )
    parser.add_argument(
        "--types",
        nargs="+",
        choices=["index", "futures", "options", "equity"],
        default=["index", "futures", "options"],
        help="Data types to transform"
    )
    parser.add_argument(
        "--lot-size-csv",
        type=Path,
        default=None,
        help="Path to CSV file containing stock lot sizes (must have 'November 2025' column)"
    )
    parser.add_argument(
        "--default-lot-size",
        type=int,
        default=1,
        help="Default lot size to use for equity symbols when not found in CSV (default: 1)"
    )
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Logging level for console/file handlers (default: INFO)"
    )
    parser.add_argument(
        "--log-file",
        type=Path,
        default=None,
        help="Log file path. Use '-' to disable file logging. Default: PROJECT_ROOT/logs/transform_nautilus.log (rotates daily)"
    )
    parser.add_argument(
        "--log-greeks-debug",
        action="store_true",
        help="Enable verbose per-record Greeks diagnostics (intended for narrow debug runs)"
    )
    parser.add_argument(
        "--expiry-dates",
        nargs="+",
        help="Limit options processing to these expiry dates (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--strike-min",
        type=float,
        help="Minimum strike (rupees) to process for options Greeks"
    )
    parser.add_argument(
        "--strike-max",
        type=float,
        help="Maximum strike (rupees) to process for options Greeks"
    )
    parser.add_argument(
        "--strike-center",
        type=float,
        help="Center strike (rupees) for symmetric strike band filtering"
    )
    parser.add_argument(
        "--strike-band",
        type=float,
        default=1000.0,
        help="Strike band width (rupees) for symmetric filtering (default: 1000)"
    )
    
    args = parser.parse_args()

    # Remote paths on SSH server (normalize to use forward slashes)
    remote_input_dir = str(args.input_dir).replace("\\", "/")
    remote_output_dir = str(args.output_dir).replace("\\", "/")
    
    # Ensure paths start with / for absolute paths
    if remote_input_dir and not remote_input_dir.startswith("/"):
        remote_input_dir = "/" + remote_input_dir
    if remote_output_dir and not remote_output_dir.startswith("/"):
        remote_output_dir = "/" + remote_output_dir

    # Temporary local directories
    local_input_dir = Path(tempfile.mkdtemp(prefix="nautilus_input_"))
    local_output_dir = Path(tempfile.mkdtemp(prefix="nautilus_output_"))
    
    logger.info(f"Downloading input from SSH server: {remote_input_dir} -> {local_input_dir}")
    logger.info(f"Output will be uploaded to SSH server: {local_output_dir} -> {remote_output_dir}")
    
    # Download input data from SSH server
    try:
        download_input_from_ssh(remote_input_dir, local_input_dir)
    except Exception as e:
        logger.error(f"Failed to download input data from SSH server: {e}", exc_info=True)
        # Clean up temp directories
        if local_input_dir.exists():
            shutil.rmtree(local_input_dir)
        if local_output_dir.exists():
            shutil.rmtree(local_output_dir)
        return 1
    
    # Configure logging
    # Set default log file if not provided and not disabled
    if args.log_file == Path('-'):
        log_file = None
    elif args.log_file is None:
        # Default log file location
        default_log_dir = PROJECT_ROOT / "logs"
        default_log_dir.mkdir(parents=True, exist_ok=True)
        log_file = default_log_dir / "transform_nautilus.log"
    else:
        log_file = args.log_file
    
    resolved_log_file = configure_logging(args.log_level, log_file)
    if resolved_log_file:
        logger.info(f"Logging to file: {resolved_log_file}")
    
    # Store resolved log file for summary
    final_log_file = resolved_log_file
    
    # Clean output directory if requested
    # Note: cleaning applies to the remote directory conceptually; the temporary
    # local directory is always new. We still log the intent for visibility.
    if args.clean:
        logger.info(f"'--clean' specified for remote output directory: {remote_output_dir} (no local data is kept)")
    
    # Construct ParquetDataCatalog using the temporary local directory
    catalog = ParquetDataCatalog(local_output_dir)
    logger.info(f"Using catalog (temporary local): {local_output_dir} -> will upload to remote: {remote_output_dir}")
    
    # Load lot sizes from CSV if provided
    resolved_csv_path = None
    if args.lot_size_csv:
        csv_path = args.lot_size_csv
        # Try to resolve the path - check multiple locations
        if not csv_path.exists():
            # Try relative to current working directory
            cwd_path = Path.cwd() / csv_path
            if cwd_path.exists():
                csv_path = cwd_path
            else:
                # Try relative to project root
                project_path = PROJECT_ROOT / csv_path
                if project_path.exists():
                    csv_path = project_path
                else:
                    # Try relative to project root with just the filename
                    project_filename = PROJECT_ROOT / csv_path.name
                    if project_filename.exists():
                        csv_path = project_filename
                    else:
                        # Try in marvelquant directory
                        marvelquant_path = PROJECT_ROOT / "marvelquant" / csv_path.name
                        if marvelquant_path.exists():
                            csv_path = marvelquant_path
                        else:
                            # Try fuzzy matching - look for files with similar names (handles spacing variations)
                            # Normalize filename by removing spaces/underscores for comparison
                            filename_base = csv_path.name.lower().replace(" ", "").replace("_", "")
                            search_dirs = [Path.cwd(), PROJECT_ROOT, PROJECT_ROOT / "marvelquant"]
                            for search_dir in search_dirs:
                                if search_dir.exists():
                                    for file in search_dir.glob("*lot*size*.csv"):
                                        file_base = file.name.lower().replace(" ", "").replace("_", "")
                                        if file_base == filename_base:
                                            csv_path = file
                                            logger.info(f"Found lot size CSV with fuzzy match: {csv_path}")
                                            break
                                    if csv_path.exists():
                                        break
        
        if csv_path.exists():
            resolved_csv_path = csv_path.resolve()  # Resolve to absolute path
            set_lot_size_csv_path(resolved_csv_path)
            logger.info(f"Loaded lot sizes from CSV: {resolved_csv_path}")
        else:
            logger.warning(f"Lot size CSV file not found: {args.lot_size_csv}")
            logger.warning(f"  Searched in: current directory, project root ({PROJECT_ROOT}), and marvelquant directory")
    
    # Parse expiry dates if provided
    expiry_filter = None
    if args.expiry_dates:
        expiry_filter = parse_expiry_dates(args.expiry_dates)
        if expiry_filter:
            logger.info(f"Expiry filter: {sorted([d.isoformat() for d in expiry_filter])}")
    
    # Handle symmetric strike filtering
    strike_min = args.strike_min
    strike_max = args.strike_max
    if args.strike_center is not None:
        strike_min = args.strike_center - args.strike_band
        strike_max = args.strike_center + args.strike_band
        logger.info(f"Symmetric strike filter: center={args.strike_center}, band=±{args.strike_band}")
    
    # Aggregate metrics across all transformations
    total_metrics = {
        'index_bars': 0,
        'futures_bars': 0,
        'options_bars': 0,
        'equity_bars': 0,
        'options_greeks': 0,
    }
    
    # Aggregate options metrics for skip rate calculation
    all_options_metrics = []
    all_date_metrics = {}
    
    # Process each symbol
    for symbol in args.symbols:
        logger.info(f"Processing symbol: {symbol}")
        
        # Transform index bars
        if "index" in args.types:
            try:
                bars_count = transform_index_bars(
                    input_dir=local_input_dir,
                    catalog=catalog,
                    symbol=symbol,
                    start_date=args.start_date,
                    end_date=args.end_date
                )
                total_metrics['index_bars'] += bars_count
            except Exception as e:
                logger.error(f"Failed to transform {symbol} index bars: {e}", exc_info=True)
        
        # Transform futures bars
        if "futures" in args.types:
            try:
                bars_count, _ = transform_futures_bars(
                    input_dir=local_input_dir,
                    catalog=catalog,
                    symbol=symbol,
                    start_date=args.start_date,
                    end_date=args.end_date,
                    output_dir=local_output_dir
                )
                total_metrics['futures_bars'] += bars_count
            except Exception as e:
                logger.error(f"Failed to transform {symbol} futures bars: {e}", exc_info=True)
        
        # Transform options bars + Greeks
        if "options" in args.types:
            try:
                bars_count, options_metrics, date_metrics = transform_options_bars(
                    input_dir=local_input_dir,
                    catalog=catalog,
                    symbol=symbol,
                    start_date=args.start_date,
                    end_date=args.end_date,
                    output_dir=local_output_dir,
                    expiry_filter=expiry_filter,
                    strike_min=strike_min,
                    strike_max=strike_max,
                    log_greeks_debug=args.log_greeks_debug
                )
                total_metrics['options_bars'] += bars_count
                all_options_metrics.append(options_metrics)
                # Merge date metrics
                for date_key, metrics in date_metrics.items():
                    if date_key not in all_date_metrics:
                        all_date_metrics[date_key] = {
                            'records_processed': 0,
                            'records_skipped': 0,
                            'skip_reasons': {}
                        }
                    all_date_metrics[date_key]['records_processed'] += metrics['records_processed']
                    all_date_metrics[date_key]['records_skipped'] += metrics['records_skipped']
                    for reason, count in metrics['skip_reasons'].items():
                        all_date_metrics[date_key]['skip_reasons'][reason] = (
                            all_date_metrics[date_key]['skip_reasons'].get(reason, 0) + count
                        )
            except Exception as e:
                logger.error(f"Failed to transform {symbol} options bars: {e}", exc_info=True)
        
        # Transform equity bars
        if "equity" in args.types:
            try:
                bars_count = transform_equity_bars(
                    input_dir=local_input_dir,
                    catalog=catalog,
                    symbol=symbol,
                    start_date=args.start_date,
                    end_date=args.end_date,
                    csv_path=resolved_csv_path
                )
                total_metrics['equity_bars'] += bars_count
            except Exception as e:
                logger.error(f"Failed to transform {symbol} equity bars: {e}", exc_info=True)
    
    # Print final summary
    logger.info("=" * 80)
    logger.info("TRANSFORMATION SUMMARY")
    logger.info("=" * 80)
    logger.info(f"Symbols processed: {', '.join(args.symbols)}")
    logger.info(f"Date range: {args.start_date} to {args.end_date}")
    logger.info(f"Types processed: {', '.join(args.types)}")
    if final_log_file:
        logger.info(f"Log file: {final_log_file}")
    logger.info(f"Total index bars: {total_metrics['index_bars']:,}")
    logger.info(f"Total futures bars: {total_metrics['futures_bars']:,}")
    logger.info(f"Total options bars: {total_metrics['options_bars']:,}")
    logger.info(f"Total equity bars: {total_metrics['equity_bars']:,}")
    
    # Calculate and display skip rate for options (if available)
    if all_date_metrics and "options" in args.types:
        total_processed = sum(m['records_processed'] for m in all_date_metrics.values())
        total_skipped = sum(m['records_skipped'] for m in all_date_metrics.values())
        total_records = total_processed + total_skipped
        
        if total_records > 0:
            skip_rate = (total_skipped / total_records) * 100.0
            logger.info("")
            logger.info("OPTIONS GREEKS SKIP RATE SUMMARY")
            logger.info("-" * 80)
            logger.info(f"Total records: {total_records:,}")
            logger.info(f"Processed: {total_processed:,}")
            logger.info(f"Skipped: {total_skipped:,}")
            logger.info(f"Skip rate: {skip_rate:.2f}%")
            
            # Aggregate skip reasons across all dates
            aggregated_skip_reasons = {}
            for date_metrics in all_date_metrics.values():
                for reason, count in date_metrics['skip_reasons'].items():
                    aggregated_skip_reasons[reason] = aggregated_skip_reasons.get(reason, 0) + count
            
            # Display top N skip reasons (default 10)
            if aggregated_skip_reasons:
                top_skip_reasons = sorted(
                    aggregated_skip_reasons.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:10]
                logger.info("")
                logger.info("Top skip reasons:")
                for reason, count in top_skip_reasons:
                    if count > 0:
                        pct = (count / total_records) * 100.0
                        logger.info(f"  {reason}: {count:,} ({pct:.2f}%)")
            
            # Warning if skip rate exceeds threshold (default 5%)
            skip_rate_threshold = 5.0
            if skip_rate > skip_rate_threshold:
                logger.warning("")
                logger.warning(f"⚠️  WARNING: Skip rate ({skip_rate:.2f}%) exceeds threshold ({skip_rate_threshold}%)")
                logger.warning("   This may indicate data quality issues. Review logs for details.")
    
    logger.info("=" * 80)

    # Upload output and log file to SSH server, then clean up temporary directories
    try:
        # Upload output to SSH server
        try:
            upload_output_to_ssh(local_output_dir, remote_output_dir)
            logger.info(f"Successfully uploaded output to SSH server: {remote_output_dir}")
        except Exception as e:
            logger.error(f"Failed to upload Nautilus catalog to SSH server: {e}", exc_info=True)
        
        # Upload log file to SSH server
        if final_log_file and final_log_file.exists():
            try:
                # Create remote log file path with timestamp
                log_filename = final_log_file.name
                remote_log_path = f"/home/ubuntu/logs/{log_filename}"
                upload_file_to_ssh(final_log_file, remote_log_path)
                logger.info(f"Log file uploaded to SSH server: {remote_log_path}")
            except Exception as e:
                logger.error(f"Failed to upload log file to SSH server: {e}", exc_info=True)
    finally:
        # Clean up temporary directories (always execute)
        if local_input_dir.exists():
            shutil.rmtree(local_input_dir)
            logger.info(f"Removed temporary local input directory: {local_input_dir}")
        if local_output_dir.exists():
            shutil.rmtree(local_output_dir)
            logger.info(f"Removed temporary local output directory: {local_output_dir}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

